{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c5ca572-36f9-48e4-9b10-1396814c57f5",
   "metadata": {},
   "source": [
    "# Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "785ef962-22ec-4eb4-aa14-7dbe9f0e7e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper on top of OpenAI\n",
    "from langchain_openai import OpenAI\n",
    "import langchain\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86bfd95f-457d-48c3-b7b5-815a073dc86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"apikey.txt\") as f:\n",
    "    my_key = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29fa8aab-93c5-40e9-9dd5-49d94aa7a0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=my_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55ffbd79-4ac9-4ac6-ae7c-b09b5886b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero shot prompting\n",
    "prompt=\"can you tell me total number of countries in aisa? can you give me top 10 country names?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8748da57-7a84-4cf1-abff-fcf64cc2d63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client.invoke(prompt).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "641446e7-d54f-4bc1-ba60-8ffe668505d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero shot prompting\n",
    "prompt2=\"Tell me the capital of India\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a2ff2532-3b43-4923-9a74-f5b366ea0394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of India is New Delhi.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.invoke(prompt2).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ee22309a-f320-47e3-ae35-f3669b51aa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3 = \"What exactly are tokens and vectors?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "216e2953-f96a-4713-9289-023c849279bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tokens and vectors are two different types of data structures commonly used in computer science and natural language processing.\\n\\n1. Tokens: In computer science, a token refers to a single element or unit of data that is recognized and processed as a single entity. In natural language processing, tokens are often referred to as words or symbols that have semantic meaning. They are created by splitting a sentence or a text into individual units, with each unit representing a word or a punctuation mark. Tokens are often used as the basic building blocks for further analysis and processing of text data.\\n\\n2. Vectors: In computer science and mathematics, a vector is a data structure that represents a quantity or a direction in space. It is defined as an ordered list of numbers or elements that can be manipulated using mathematical operations such as addition, subtraction, and multiplication. In natural language processing, vectors are often used to represent words or phrases in a high-dimensional space, where each dimension represents a feature or aspect of the word or phrase. These word vectors are then used in various language models and algorithms for tasks such as word similarity, sentence classification, and text generation.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.invoke(prompt3).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c236eaf1-402c-4577-934d-d11b072f906a",
   "metadata": {},
   "source": [
    "# Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "940db250-13c5-4d3c-95bc-97d3089a15ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ed7f003c-57fe-463d-a41e-2ffc22db493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables = ['country'],\n",
    "    template = \"Tell me the capital of {country}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5582117c-a75b-40be-b3a9-b1046c257198",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = prompt_template_name.format(country='India')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f01b4bc9-d4a2-41d2-96ce-55569e061825",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = prompt_template_name.format(country='China')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9b391a76-13b1-41e8-8374-77d5bbb1946e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of India is New Delhi.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.invoke(prompt1).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5a274502-3075-4a13-a5d9-562f4a2b2f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Beijing'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.invoke(prompt2).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "62f88fe0-d149-45ce-a36d-f1750ca5f4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2\n",
    "prompt3 = PromptTemplate.from_template(\"Suggest me a good name for a company that makes {product}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bad7c57d-a691-4b32-9c4b-75b748f0749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3 = prompt3.format(product='toys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fc62a277-5a75-4567-9a7b-dff59aca6407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Playful Creations\"'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.invoke(prompt3).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5fb227-e7d8-4c58-8dc1-e774fb8cc6b5",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc258a45-c1c1-4aeb-b679-ac5732183077",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt4 = \"Tell me which team won the recent Cricket World Cup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2cab30c-7894-4db4-b02d-b670f2121b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The England cricket team won the recent Cricket World Cup, defeating the New Zealand cricket team in a thrilling final match.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.invoke(prompt4).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "419c967a-d077-4570-a2cf-331fb784370e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As of 2021, the GDP of India is approximately $2.7 trillion, making it the fifth largest economy in the world.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt5 = \"Tell me the GDP of India\"\n",
    "client.invoke(prompt5).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd61542-b31b-4cf6-bed1-3cdd678dcc5a",
   "metadata": {},
   "source": [
    "### For getting real time info using serp API. By using Serp API, call the google search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c94af40b-39a8-4175-89b4-f7183e8587e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"serpapikey.txt\") as f:\n",
    "    serp_api_key = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3d28d0a-47d5-4708-84da-c0429f6b9eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, load_tools, initialize_agent\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c2bdc46-e119-4c23-bff6-6f63588a5fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=my_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a1fa1ee-7333-456c-bf6b-1a103910dd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = load_tools(['serpapi'], llm=client, serpapi_api_key=serp_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "131e18f0-1b1a-40c3-ab29-b3ac5db080e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Narex\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "agent = initialize_agent(tool, client, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3cd5514c-b3cd-4fa3-9525-411e4bdebde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Narex\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should search for the results of the Cricket World Cup\n",
      "Action: Search\n",
      "Action Input: \"Cricket World Cup results\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[\"Official source of all upcoming ICC Cricket Men's and Women's fixtures and results. Filter by team, event, date, location and format (Test, First Class, ...\", 'Help: ICC World Cup scores service offers scores, standings, upcoming matches, ICC World Cup final results and match details. Follow ICC World Cup scoreboard, ...', 'Australia has won the tournament six times, India and West Indies twice each, while Pakistan, Sri Lanka and England have won it once each. The best performance ...', 'Check out the complete results from the 2023 Cricket World Cup as Australia triumphed for a sixth time and England were dumped out in the group ...', 'World Cup trophy team series results, cricket records by trophy only on ESPNcricinfo.Check stats and records of World Cup trophy team series results.', 'Flashscore.com offers ICC World Cup livescore, partial and final results, statistics and scorecards. Besides ICC World Cup scores you can follow 5000+ ...', 'Check World Cup 2023 live score 2023/24, squads, match schedules, World Cup 2023 points table, fixtures, updates, photos, and videos on ESPNcricinfo.', 'Most recently, the 2023 Cricket World Cup, hosted by India, was won by Australia, who beat India. The current trophy was instituted in 1999. It always remains ...', 'Flashscore.com provides all ICC World Cup final results, live scores and upcoming matches with current standings, head-to-head stats and odds comparison. See ...', 'The last three World Cups have all been won by the hosting nations -- India 2011, Australia (co-hosts alongside New Zealand) 2015 and England 2019. A total of ...']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should look for the final results of the Cricket World Cup\n",
      "Action: Search\n",
      "Action Input: \"Cricket World Cup final results\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['Most recently, the 2023 Cricket World Cup, hosted by India, was won by Australia, who beat India. The current trophy was instituted in 1999. It always remains ...', 'Australia are the current champions after winning the 2023 World Cup in India. The subsequent 2027 World Cup will be held jointly in South Africa, Zimbabwe and ...', 'World Cup trophy team series results, cricket records by trophy only on ESPNcricinfo.Check stats and records of World Cup trophy team series results.', 'A list of Cricket World Cup finals: 2023 in India: Australia defeated India by six wickets. 2019 in England:.', \"Results of the Cricket World Cup are provided in the table. ... World Cup in the finals · FIFA Men's World Cup ... Uruguay scoring its first goal in ...\", 'Sri Lanka beat England by five wickets in the quarter-finals, defeated India by 131 runs in the semi-finals and then handed Australia a seven- ...', 'England won by 93 runs. Australia. Bangladesh. 307/2 (44.4). 306/8 (50).', 'Australia won the 13th edition of the ICC ODI World Cup as they defeated India by 6 wickets at the Narendra Modi Stadium in Ahmedabad.', 'The previous champion was England, who defeated New Zealand in the immensely enthralling final of the 2019 World Cup. With England looking ...']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should look for the specific World Cup final results for the recent Cricket World Cup\n",
      "Action: Search\n",
      "Action Input: \"Cricket World Cup 2023 final results\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'title': 'ICC Cricket World Cup', 'thumbnail': 'https://serpapi.com/searches/66152381ef7ddab69a344586/images/68ffa929fe805a955dbd77025dfe3edc2e8006ed477308fdcba69215b28f819d.png', 'games': [{'tournament': 'ICC Cricket World Cup', 'stadium': 'Narendra Modi Stadium', 'date': 'Nov 19, 23', 'teams': [{'name': 'India', 'score': '240 (50)', 'thumbnail': 'https://serpapi.com/searches/66152381ef7ddab69a344586/images/68ffa929fe805a95a4ac7ccdc99a04a2b3029fae90474ba34d6cd16dab27f3559a923fb9cf77438382366a5edd544aed.png'}, {'name': 'Australia', 'score': '241/4 (43)', 'thumbnail': 'https://serpapi.com/searches/66152381ef7ddab69a344586/images/68ffa929fe805a95a4ac7ccdc99a04a2b3029fae90474ba3eea2b7706d68aa9a515e39b7ace3e22b603092357957e32f.png'}], 'status': 'AUS won by 6 wickets (42 balls left)'}, {'tournament': 'ICC Cricket World Cup', 'stadium': 'Eden Gardens', 'date': 'Nov 16, 23', 'teams': [{'name': 'South Africa', 'score': '212 (49.4)', 'thumbnail': 'https://serpapi.com/searches/66152381ef7ddab69a344586/images/68ffa929fe805a95a4ac7ccdc99a04a21c5d411d701ed960e1dae6c23e161a8b4e678da7414bc752e200ce93d0dbcbbb.png'}, {'name': 'Australia', 'score': '215/7 (47.2)', 'thumbnail': 'https://serpapi.com/searches/66152381ef7ddab69a344586/images/68ffa929fe805a95a4ac7ccdc99a04a21c5d411d701ed9606c811f0550466b2721168d446afbf4aa5a6ab9d5eec3bffe.png'}], 'status': 'AUS won by 3 wickets (16 balls left)'}, {'tournament': 'ICC Cricket World Cup', 'stadium': 'Wankhede Stadium', 'date': 'Nov 15, 23', 'teams': [{'name': 'India', 'score': '397/4 (50)', 'thumbnail': 'https://serpapi.com/searches/66152381ef7ddab69a344586/images/68ffa929fe805a95a4ac7ccdc99a04a26f32f377731d283303234c58e8de500273b8979e3ed9b3a0dbc82c1b16599e75.png'}, {'name': 'New Zealand', 'score': '327 (48.5)', 'thumbnail': 'https://serpapi.com/searches/66152381ef7ddab69a344586/images/68ffa929fe805a95a4ac7ccdc99a04a26f32f377731d28334d60154688acf00c616711847d5fb17b492d725ea9d6cd07.png'}], 'status': 'IND won by 70 runs'}, {'tournament': 'ICC Cricket World Cup', 'stadium': 'M. Chinnaswamy Stadium', 'date': 'Nov 12, 23', 'teams': [{'name': 'India', 'score': '410/4 (50)', 'thumbnail': 'https://serpapi.com/searches/66152381ef7ddab69a344586/images/68ffa929fe805a95a4ac7ccdc99a04a2a1337eb0d7f4b240bbf85fff83d2fc4a5a13327d351a8ae914d1779b52824b7c.png'}, {'name': 'Netherlands', 'score': '250 (47.5)', 'thumbnail': 'https://serpapi.com/searches/66152381ef7ddab69a344586/images/68ffa929fe805a95a4ac7ccdc99a04a2a1337eb0d7f4b240799c56e86cc1f4425e42c1be9fa66807d29b280e9fc002a4.png'}], 'status': 'IND won by 160 runs'}, {'tournament': 'ICC Cricket World Cup', 'stadium': 'Eden Gardens', 'date': 'Nov 11, 23', 'teams': [{'name': 'England', 'score': '337/9 (50)', 'thumbnail': 'https://serpapi.com/searches/66152381ef7ddab69a344586/images/68ffa929fe805a95a4ac7ccdc99a04a27053adae7c0dd144583eb510bf8e6f644e14422addad7b7f3df9e3affdc125a5.png'}, {'name': 'Pakistan', 'score': '244 (43.3)', 'thumbnail': 'https://serpapi.com/searches/66152381ef7ddab69a344586/images/68ffa929fe805a95a4ac7ccdc99a04a27053adae7c0dd144c0c60164c9d403340c6b39d36ec4c9598241152ce696c352.png'}], 'status': 'ENG won by 93 runs'}, {'tournament': 'ICC Cricket World Cup', 'stadium': 'Maharashtra Cricket Association Stadium', 'date': 'Nov 11, 23', 'teams': [{'name': 'Bangladesh', 'score': '306/8 (50)', 'thumbnail': 'https://serpapi.com/searches/66152381ef7ddab69a344586/images/68ffa929fe805a95a4ac7ccdc99a04a23b42c8496479191a70fe6d1f8f740f8fb1afbff08c428518b0c28c32bc3c92a0.png'}, {'name': 'Australia', 'score': '307/2 (44.4)', 'thumbnail': 'https://serpapi.com/searches/66152381ef7ddab69a344586/images/68ffa929fe805a95a4ac7ccdc99a04a23b42c8496479191a2c86288b0032a5428c9d93f632ef9e4743a8c58ee0cad98a.png'}], 'status': 'AUS won by 8 wickets (32 balls left)'}]}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The recent Cricket World Cup was won by Australia in the final match against India with a score of 241/4 (43).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The recent Cricket World Cup was won by Australia in the final match against India with a score of 241/4 (43).'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Tell me which team won the recent Cricket World Cup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "362fb6fe-991a-4d69-99c0-bc52df9ab1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should think about what to do\n",
      "Action: Search\n",
      "Action Input: \"top 5 current affairs\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['Here we are providing the daily Current Affairs 2024 for upcoming Bank, SSC, Railway and UPSC Exams. It will improve the current affairs and General ...', 'Current Affairs · Srisailam Temple Comes Alive with Prabhotsavam Festivities · AP: Womens Commission seeks report on girl students safety · Since the formation of ...', 'Latest Current Affairs · National Current Affairs · International Current Affairs · Sports Current Affairs · Technology Current Affairs · Economy Current Affairs ...', \"GKToday's Current Affairs Today Section provides the latest and Best Daily Current Affairs 2024-25 for UPSC, IAS/PCS, Banking, IBPS, SSC, Railway, UPPSC, ...\", 'Get daily updates on latest Current Affairs 2024 for upcoming Bank, SSC, Railway, Defence, UPSC, UPPSC, BPSC, MPPSC, RPSC, KPSC, & all other Govt ...', 'Latest world news headlines: International breaking news and current affairs from US, Europe, Middle East, Asia, Africa and more. If it happens around the ...', 'Adda247 provides you with the best compilation of National Current Affairs & News. Find the most relevant and updated India National Current Affairs for Banking ...', 'Check out for the latest news on current affairs along with current affairs live news at Times of India.', 'A Current Affairs subscription is one of the best known ways to improve your life in a hurry. Our print magazine is released six times a year, in a beautiful ...']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should think about what to do next\n",
      "Action: Search\n",
      "Action Input: \"top 5 current affairs for today\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['Latest world news headlines: International breaking news and current affairs from US, Europe, Middle East, Asia, Africa and more. If it happens around the ...', 'Reuters.com is your online source for the latest US news stories and current events, ensuring our readers up to date with any breaking news developments.', 'USA TODAY delivers current national and local news, sports, entertainment, finance, technology, and more through award-winning journalism, photos, ...', 'Get the latest news about politics, the economy, health care, the environment, education, Congress and state, local and federal government.', 'View the latest news and breaking news today for U.S., world, weather, entertainment, politics and health at CNN.com.', 'Current Affairs ; TSMC Acts against 19 Quacks · 2 April 2024 10:00 AM ; Retail Car Sales Rise 8.45% in FY24: FADA · 8 April 2024 10:20 PM ; Srisailam Temple Comes ...', 'Your trusted source for breaking news, analysis, exclusive interviews, headlines, and videos at ABCNews.com.', 'View CNN world news today for international news and videos from Europe, Asia, Africa, the Middle East and the Americas.', 'US News has breaking world news on the war in Iraq and Afghanistan, international business news about the European and Asian economic market, and more.', 'A Current Affairs subscription is one of the best known ways to improve your life in a hurry. Our print magazine is released six times a year, in a beautiful ...']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The top 5 current affairs for today are: 1) International breaking news, 2) Latest US news stories and current events, 3) National and local news from USA TODAY, 4) Politics, economy, and government news, and 5) World news and videos from CNN.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The top 5 current affairs for today are: 1) International breaking news, 2) Latest US news stories and current events, 3) National and local news from USA TODAY, 4) Politics, economy, and government news, and 5) World news and videos from CNN.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Tell me top 5 current affairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "045f3d69-1631-497d-9adf-f363617404d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b4704ff-efc5-41e4-ab7d-4efe3bcfe7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = load_tools(['wikipedia'], llm=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ebc31865-d6a6-4814-a377-f966122f46bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(tool, client, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a5344ea-2349-4d1a-9972-3b4ff4b0f725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should find information about the Apollo 11 mission\n",
      "Action: wikipedia\n",
      "Action Input: Apollo 11\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Narex\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\Narex\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Apollo 11\n",
      "Summary: Apollo 11 (July 16–24, 1969) was the American spaceflight that first landed humans on the Moon. Commander Neil Armstrong and Lunar Module Pilot Buzz Aldrin landed the Apollo Lunar Module Eagle on July 20, 1969, at 20:17 UTC, and Armstrong became the first person to step onto the Moon's surface six hours and 39 minutes later, on July 21 at 02:56 UTC. Aldrin joined him 19 minutes later, and they spent about two and a quarter hours together exploring the site they had named Tranquility Base upon landing. Armstrong and Aldrin collected 47.5 pounds (21.5 kg) of lunar material to bring back to Earth as pilot Michael Collins flew the Command Module Columbia in lunar orbit, and were on the Moon's surface for 21 hours, 36 minutes before lifting off to rejoin Columbia.\n",
      "Apollo 11 was launched by a Saturn V rocket from Kennedy Space Center on Merritt Island, Florida, on July 16 at 13:32 UTC, and it was the fifth crewed mission of NASA's Apollo program. The Apollo spacecraft had three parts: a command module (CM) with a cabin for the three astronauts, the only part that returned to Earth; a service module (SM), which supported the command module with propulsion, electrical power, oxygen, and water; and a lunar module (LM) that had two stages—a descent stage for landing on the Moon and an ascent stage to place the astronauts back into lunar orbit.\n",
      "After being sent to the Moon by the Saturn V's third stage, the astronauts separated the spacecraft from it and traveled for three days until they entered lunar orbit. Armstrong and Aldrin then moved into Eagle and landed in the Sea of Tranquility on July 20. The astronauts used Eagle's ascent stage to lift off from the lunar surface and rejoin Collins in the command module. They jettisoned Eagle before they performed the maneuvers that propelled Columbia out of the last of its 30 lunar orbits onto a trajectory back to Earth. They returned to Earth and splashed down in the Pacific Ocean on July 24 after more than eight days in space.\n",
      "Armstrong's first step onto the lunar surface was broadcast on live TV to a worldwide audience. He described the event as \"one small step for [a] man, one giant leap for mankind.\" Apollo 11 effectively proved U.S. victory in the Space Race to demonstrate spaceflight superiority, by fulfilling a national goal proposed in 1961 by President John F. Kennedy, \"before this decade is out, of landing a man on the Moon and returning him safely to the Earth.\"\n",
      "\n",
      "Page: Apollo 11 (2019 film)\n",
      "Summary: Apollo 11 is a 2019 American documentary film edited, produced and directed by Todd Douglas Miller. It focuses on the 1969 Apollo 11 mission, the first spaceflight from which men walked on the Moon. The film consists solely of archival footage, including 70 mm film previously unreleased to the public, and does not feature narration, interviews or modern recreations. The Saturn V rocket, Apollo crew consisting of Buzz Aldrin, Neil Armstrong, and Michael Collins, and Apollo program Earth-based mission operations engineers are prominently featured in the film.The film premiered at the Sundance Film Festival on January 24, 2019, and was released theatrically in the United States by Neon on March 1, 2019. Apollo 11 received acclaim from critics and grossed $15 million. The film won a leading five awards at the 4th Critics' Choice Documentary Awards, including Best Documentary Feature. It also received five nominations at the 72nd Primetime Creative Arts Emmy Awards, including Outstanding Cinematography for a Nonfiction Program for Aldrin and Collins, winning three for its editing and sound.\n",
      "\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The Apollo 11 mission was the first successful landing of humans on the moon, accomplished by the United States in 1969. The mission was a part of the Apollo program, proposed by President John F. Kennedy in 1961 to demonstrate U.S. spaceflight superiority. The mission was carried out by astronauts Neil Armstrong, Buzz Aldrin, and Michael Collins, and was documented in the 2019 film Apollo 11.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Apollo 11 mission was the first successful landing of humans on the moon, accomplished by the United States in 1969. The mission was a part of the Apollo program, proposed by President John F. Kennedy in 1961 to demonstrate U.S. spaceflight superiority. The mission was carried out by astronauts Neil Armstrong, Buzz Aldrin, and Michael Collins, and was documented in the 2019 film Apollo 11.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Tell me about the moon landing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "132be416-fa68-4dc7-b015-641d22db889e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should first read about GDP\n",
      "Action: wikipedia\n",
      "Action Input: GDP\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Gross domestic product\n",
      "Summary: Gross domestic product (GDP) is a monetary measure of the market value of all the final goods and services produced in a specific time period by a country or countries. GDP is more often used by the government of a single country to measure its economic health. Due to its complex and subjective nature, this measure is often revised before being considered a reliable indicator.GDP definitions are maintained by several national and international economic organizations. The Organisation for Economic Co-operation and Development (OECD) defines GDP as \"an aggregate measure of production equal to the sum of the gross values added of all resident and institutional units engaged in production and services (plus any taxes, and minus any subsidies, on products not included in the value of their outputs)\". An IMF publication states that, \"GDP measures the monetary value of final goods and services—that are bought by the final user—produced in a country in a given period (say a quarter or a year).\"GDP (nominal) per capita does not reflect differences in the cost of living and the inflation rates of the countries; therefore, using a basis of GDP per capita at purchasing power parity (PPP) may be more useful when comparing living standards between nations, while nominal GDP is more useful comparing national economies on the international market. Total GDP can also be broken down into the contribution of each industry or sector of the economy. The ratio of GDP to the total population of the region is the per capita GDP (also called the Mean Standard of Living).\n",
      "GDP is often used as a metric for international comparisons as well as a broad measure of economic progress. It is often considered to be the world's most powerful statistical indicator of national development and progress. However, critics of the growth imperative often argue that GDP measures were never intended to measure progress, and leave out key other externalities, such as resource extraction, environmental impact and unpaid domestic work. Critics frequently propose alternative economic models such as doughnut economics which use other measures of success or alternative indicators such as the OECD's Better Life Index as better approaches to measuring the effect of the economy on human development and well being.\n",
      "For example, the GDP of Germany in 2023 was 4100 Billion Euro, which included about 10% of taxes like the value-added tax.\n",
      "\n",
      "Page: List of countries by GDP (nominal)\n",
      "Summary: Gross domestic product (GDP) is the market value of all final goods and services from a nation in a given year. Countries are sorted by nominal GDP estimates from financial and statistical institutions, which are calculated at market or government official exchange rates. Nominal GDP does not take into account differences in the cost of living in different countries, and the results can vary greatly from one year to another based on fluctuations in the exchange rates of the country's currency. Such fluctuations may change a country's ranking from one year to the next, even though they often make little or no difference in the standard of living of its population.Comparisons of national wealth are also frequently made based on purchasing power parity (PPP), to adjust for differences in the cost of living in different countries. Other metrics, nominal GDP per capita and a corresponding GDP (PPP) per capita are used for comparing national standard of living. On the whole, PPP per capita figures are less spread than nominal GDP per capita figures.The rankings of national economies over time have changed considerably; the United States surpassed the British Empire's output around 1916, which in turn had surpassed the Qing dynasty in aggregate output decades earlier. Since China's transition to a socialist market economy through controlled privatisation and deregulation, the country has seen its ranking increase from ninth in 1978, to second in 2010; China's economic\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The United States has a GDP of 4100 Billion Euro, ranking second in the world behind China. GDP is a measure of the market value of all final goods and services produced in a country in a given time period. It is often used as a metric for international comparisons and economic progress. However, critics argue that it does not account for other important factors such as resource extraction, environmental impact, and unpaid domestic work.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The United States has a GDP of 4100 Billion Euro, ranking second in the world behind China. GDP is a measure of the market value of all final goods and services produced in a country in a given time period. It is often used as a metric for international comparisons and economic progress. However, critics argue that it does not account for other important factors such as resource extraction, environmental impact, and unpaid domestic work.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"USA GDP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698af005-9cbe-4e79-ba35-5740474306aa",
   "metadata": {},
   "source": [
    "# Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "562032f0-0270-43fc-8ac2-9750168e08f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d25c4894-accd-400b-ae75-6b8b10669df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1\n",
    "prompt = PromptTemplate.from_template(\"Suggest me a good name for a company that makes {product}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "be5cb8f7-b23e-4b0e-b875-e1a7bde488a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "19f71fd3-5844-4dcc-a9f1-ef8c67fed5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm = client, prompt = prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c299f4da-1c8c-4e23-8091-1242528dc02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"VinoVeritas\" (Latin for \"wine truth\")\n"
     ]
    }
   ],
   "source": [
    "print(chain.run('wine').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9015dd58-d6d7-4a3a-9303-9eaf461e6321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables = ['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest me a fancy name for it\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1a38509c-2277-40e8-b76d-a076b0841571",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm = client, prompt = prompt_template_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "176aada5-9b6e-4939-8273-b418d92fb534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Pasta Paradiso\"\n"
     ]
    }
   ],
   "source": [
    "print(chain.run('Italian').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bd6b431c-509c-4648-9244-e8aa6ebcecee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Thai Temptations\"\n"
     ]
    }
   ],
   "source": [
    "print(chain.run('Thai').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e9708c8e-0698-46dc-8049-e49064b48660",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm = client, prompt = prompt_template_name, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "89cab206-97ac-476f-85d7-2c1ccfc6b020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI want to open a restaurant for Japanese food. Suggest me a fancy name for it\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\"Sakura Garden\"\n"
     ]
    }
   ],
   "source": [
    "print(chain.run('Japanese').strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18af3a69-65dd-40cd-805f-839731a1e45f",
   "metadata": {},
   "source": [
    "# Sequential Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef953e90-8ec0-4e04-8d4d-f1699603f2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to combine multiple chain and set a sequence for that we use simple sequential chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b171536d-2ec2-4557-8258-ceb2fc56303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables = ['startup_name'],\n",
    "    template = \"I want to start a startup for {startup_name}, suggest me a good name for it\"\n",
    ")\n",
    "\n",
    "name_chain = LLMChain(llm = client, prompt = prompt_template_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1901e434-850b-42a3-90d2-6d39b71a1e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['name'],\n",
    "    template = \"Suggest some strategies for {name}\"\n",
    ")\n",
    "\n",
    "strategies_chain = LLMChain(llm = client, prompt = prompt_template_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "20bd4f6b-8aeb-49b4-b945-d3ec1dbfe52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9b9ee5aa-0c47-43cf-bd47-f8a2562d4a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = SimpleSequentialChain(chains=[name_chain, strategies_chain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "75414c6a-49af-42c2-a544-f0c96ad43964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Building a strong brand identity: Develop a unique and memorable brand image for IntelliGenius. This could involve creating a logo, tagline, and visual elements that represent the company's values and mission.\n",
      "\n",
      "2. Utilizing social media: Use social media platforms such as Instagram, Twitter, and LinkedIn to reach and engage with potential customers. Share interesting and relevant content, run campaigns and promotions, and interact with followers to build a strong online presence.\n",
      "\n",
      "3. Partnering with schools and educational institutions: Collaborate with schools and educational organizations to introduce IntelliGenius's products and services to students, teachers, and parents. This can include sponsoring events, offering discounts, or providing free trials to build brand awareness and credibility.\n",
      "\n",
      "4. Offering free trials and demos: Allow potential customers to try out IntelliGenius's products and services for free. This will give them a chance to experience the benefits and value of the company's offerings, which can lead to more sales in the future.\n",
      "\n",
      "5. Targeted advertising: Use targeted advertising methods such as Google AdWords, Facebook Ads, and LinkedIn Ads to reach a specific audience based on their interests, demographics, and behaviors. This can help increase brand visibility and attract potential customers.\n",
      "\n",
      "6. Creating informative and helpful content: Develop and\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"Artificial Intelligence\").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "25f870cd-030e-435b-ae89-7ca2a919d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables = ['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest me a fancy name for it\"\n",
    ")\n",
    "\n",
    "name_chain = LLMChain(llm = client, prompt = prompt_template_name, output_key='restaurant_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5c63fc20-bd41-4dae-9fd3-063f690c01fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template = \"Suggest some menu items for {restaurant_name}\"\n",
    ")\n",
    "\n",
    "food_items_chain = LLMChain(llm = client, prompt = prompt_template_items, output_key='menu_items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a78a3815-e2df-4b81-9efe-aa0a869fb946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f5d3f492-79ac-41cf-a699-fa88c97fa0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = SequentialChain(chains=[name_chain, food_items_chain],\n",
    "                       input_variables=['cuisine'],\n",
    "                       output_variables=['restaurant_name', 'menu_items'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e578a74d-de82-4836-a4c2-c0b82729a77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke({'cuisine':'Japanese'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "13c54226-5fb7-4473-982e-37cef1d711de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cuisine', 'restaurant_name', 'menu_items'])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e5da6d6c-9ee5-417a-9345-a3452968f81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Japanese'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['cuisine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "214bcbd5-2fd7-4de7-a5fa-b1ec580f2ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Samurai Sushi & Noodle Co.\"\n"
     ]
    }
   ],
   "source": [
    "print(output['restaurant_name'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "afc9108f-cf80-407e-8802-2d0bf97f7248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Sushi Rolls:\n",
      "- California Roll (crab, avocado, cucumber)\n",
      "- Dragon Roll (eel, avocado, cucumber, topped with avocado)\n",
      "- Spicy Tuna Roll (tuna, spicy mayo, cucumber)\n",
      "- Salmon Avocado Roll (salmon, avocado, cucumber)\n",
      "- Vegetarian Roll (avocado, cucumber, carrot, bell pepper)\n",
      "\n",
      "2. Noodle Dishes:\n",
      "- Ramen (choice of chicken, pork, or vegetarian broth with choice of protein and toppings)\n",
      "- Yakisoba (stir-fried egg noodles with choice of protein and vegetables)\n",
      "- Udon Soup (thick wheat noodles in a savory broth with choice of protein and vegetables)\n",
      "- Pad Thai (Thai-style rice noodles with choice of protein, egg, and peanuts)\n",
      "\n",
      "3. Small Plates:\n",
      "- Gyoza (pan-fried dumplings with choice of pork or vegetable filling)\n",
      "- Edamame (steamed soybeans with sea salt)\n",
      "- Tempura (assorted vegetables or shrimp fried in a light batter)\n",
      "- Takoyaki (fried octopus balls)\n",
      "- Spicy Tuna Tartare (diced tuna mixed with spicy mayo and served on a crispy rice cake)\n",
      "\n",
      "4. Bento Boxes:\n",
      "- Teriyaki Chicken Bento (\n"
     ]
    }
   ],
   "source": [
    "print(output['menu_items'].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607d48dc-a4cd-40ee-8d4d-e67f62ab7b74",
   "metadata": {},
   "source": [
    "# Document Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0f3a4266-7ff2-4c1e-9ccb-54b0e8e35353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2f0b7823-df01-40ff-8353-735352ee6d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(r\"C:\\Users\\Narex\\Downloads\\Experimental_Strategies\\MM-Research-Papers\\main.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bf26850d-7a9f-490b-aa33-742dd7cc2079",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fe9b8ce2-7605-455a-b825-ccea55052c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/341104642\\nRobust Market Making via Adversarial Reinforcement Learning\\nConf erence Paper  · April 2020\\nDOI: 10.24963/ ijcai.2020/626\\nCITATIONS\\n43READS\\n346\\n2 author s, including:\\nThomas Spooner\\n17 PUBLICA TIONS \\xa0\\xa0\\xa0157 CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll c ontent f ollo wing this p age was uplo aded b y Thomas Spooner  on 02 May 2020.\\nThe user has r equest ed enhanc ement of the do wnlo aded file.', metadata={'source': 'C:\\\\Users\\\\Narex\\\\Downloads\\\\Experimental_Strategies\\\\MM-Research-Papers\\\\main.pdf', 'page': 0}),\n",
       " Document(page_content='Robust Market Making via Adversarial Reinforcement Learning\\nThomas Spooner and Rahul Savani\\nDepartment of Computer Science, University of Liverpool\\n{t.spooner, rahul.savani }@liverpool.ac.uk\\nAbstract\\nWe show that adversarial reinforcement learning\\n(ARL) can be used to produce market mark-\\ning agents that are robust toadversarial and\\nadaptively-chosen market conditions. To apply\\nARL, we turn the well-studied single-agent model\\nof Avellaneda and Stoikov [2008 ]into a discrete-\\ntime zero-sum game between a market maker and\\nadversary. The adversary acts as a proxy for other\\nmarket participants that would like to proﬁt at the\\nmarket maker’s expense. We empirically com-\\npare two conventional single-agent RL agents with\\nARL, and show that our ARL approach leads to:\\n1) the emergence of risk-averse behaviour with-\\nout constraints or domain-speciﬁc penalties; 2) sig-\\nniﬁcant improvements in performance across a set\\nof standard metrics, evaluated with or without an\\nadversary in the test environment, and; 3) im-\\nproved robustness to model uncertainty. We em-\\npirically demonstrate that our ARL method consis-\\ntently converges, and we prove for several special\\ncases that the proﬁles that we converge to corre-\\nspond to Nash equilibria in a simpliﬁed single-stage\\ngame.\\n1 Introduction\\nMarket making refers to the act of providing liquidity to a\\nmarket by continuously quoting prices to buy and sell a given\\nﬁnancial instrument. The difference between these prices\\nis called a spread , and the goal of a market maker (MM)\\nis to repeatedly earn a spread by transacting in both direc-\\ntions. However, this is not done without risk. Market makers\\nexpose themselves to adverse selection , where toxic agents\\nexploit their technological or informational edge, transacting\\nwith and thereby changing the MM’s inventory before an ad-\\nverse price move causes them a loss. This is known as inven-\\ntory risk, and has been the subject of a great deal of research\\nin optimal control, artiﬁcial intelligence, and reinforcement\\nlearning (RL) literature.\\nA standard assumption in existing work has been that the\\nMM has perfect knowledge of market conditions. Robustness\\nof market making strategies to model ambiguity has only re-\\ncently received attention - Cartea et al. [2017 ]extend optimalcontrol approaches for the market making problem to address\\nthe risk of model misspeciﬁcation. In this paper, we deal with\\nthis type of risk by taking an adversarial approach. We design\\nmarket making agents that are robust to adversarial and adap-\\ntively chosen market conditions by applying adversarial RL.\\nOur starting point is a well-known single-agent mathematical\\nmodel of market making of Avellaneda and Stoikov [2008 ],\\nwhich has been used extensively in the quantitative ﬁnance\\n[Cartea et al. , 2015; Cartea et al. , 2017; Gu ´eant et al. , 2011;\\nGu´eant, 2017 ]. We convert this into a discrete-time game,\\nwith a new “market player”, the adversary, that can be thought\\nof as a proxy for other market participants that would like to\\nproﬁt at the expense of the market maker. The adversary con-\\ntrols dynamics of the market environment in a zero-sum game\\nagainst the market maker.\\nWe evaluate the sensitivity of RL-based strategies to three\\ncore parameters of the market model dynamics that affect\\nprices and execution, where each of these parameters natu-\\nrally varies over time in real markets. We thus go beyond\\nthe ﬁxed parametrisation of existing models — henceforth\\ncalled the FIXED setting — with two extended learning set-\\ntings. The RANDOM setting initialises each instance of the\\nmodel (that is, episode in RL terminology) with values for the\\nthree parameters that are chosen independently and uniformly\\nat random from three appropriate intervals. The STRATE -\\nGIC setting features an adversary – an independent “mar-\\nket” learner whose objective is to choose the parameters from\\nthese same intervals so as to minimise the cumulative reward\\nof the market maker in a zero-sum game. The R ANDOM and', metadata={'source': 'C:\\\\Users\\\\Narex\\\\Downloads\\\\Experimental_Strategies\\\\MM-Research-Papers\\\\main.pdf', 'page': 1}),\n",
       " Document(page_content='ket” learner whose objective is to choose the parameters from\\nthese same intervals so as to minimise the cumulative reward\\nof the market maker in a zero-sum game. The R ANDOM and\\nSTRATEGIC settings are, on the one hand, more realistic than\\nthe F IXED setting, but on the other hand, signiﬁcantly more\\ncomplex for the market making agent. We show that mar-\\nket making strategies trained in each of these settings yield\\nsigniﬁcantly different behaviour, and we demonstrate strik-\\ning beneﬁts of our S TRATEGIC setting.\\n1.1 Contributions\\nThe key contributions of this paper are as follows:\\n• We introduce a game-theoretic adaptation of a standard\\nmathematical model of market making. Our adaption is\\nuseful to train robust MMs, and evaluate their performance\\nin the presence of epistemic risk (Sections 2 and 3).\\n• We propose an algorithm for adversarial reinforcement\\nlearning in the spirit of RARL [Pinto et al. , 2017 ], and', metadata={'source': 'C:\\\\Users\\\\Narex\\\\Downloads\\\\Experimental_Strategies\\\\MM-Research-Papers\\\\main.pdf', 'page': 1}),\n",
       " Document(page_content='demonstrate its effectiveness in spite of the well known\\nchallenges associated with ﬁnding Nash equilibria of\\nstochastic games [Littman, 1994 ](Sections 4 and 5).\\n• We thoroughly investigate the impact of three environ-\\nmental settings (one adversarial) on learning market mak-\\ning. We show that training against a S TRATEGIC adversary\\nstrictly dominates the other two settings (F IXED and R AN-\\nDOM ) in terms of a set of standard desiderata, including the\\nSharpe ratio (Section 5).\\n• We prove that, in several key instances of the S TRATE -\\nGICsetting, the single-stage instantiation of our game has a\\nNash equilibrium resembling that found by our ARL algo-\\nrithm for the multi-stage game. We then conﬁrm broader\\nexistence of (approximate) equilibria in the multi-stage\\ngame by empirical best response computations (Sections 3\\nand 5).\\n1.2 Related work\\nOptimal control and market making. The theoretical\\nstudy of market making originated from the pioneering work\\nof Ho and Stoll [1981 ], Glosten and Milgrom [1985 ]and\\nGrossman and Miller [1988 ], among others. Subsequent\\nwork focused on characterising optimal behaviour under dif-\\nferent market dynamics and contexts. Most relevant is the\\nwork of Avellaneda and Stoikov [2008 ], who incorporated\\nnew insights into the dynamics of the limit order book to give\\na new market model, which is the one used in this paper. They\\nderived closed-form expressions for the optimal strategy of\\nan MM with an exponential utility function when the MM has\\nperfect knowledge of the model and its parameters. This same\\nproblem was then studied for other utility functions [Fodra\\nand Labadie, 2012; Gu ´eant et al. , 2011; Cartea et al. , 2015;\\nCartea and Jaimungal, 2015; Gu ´eant, 2017 ]. As mentioned\\nabove, Cartea et al. [2017 ]study the impact of uncertainty in\\nthe model of Avellaneda and Stoikov [2008 ]: they drop the\\nassumption of perfect knowledge of market dynamics, and\\nconsider how an MM should optimally trade while being ro-\\nbust to possible misspeciﬁcation. This type of epistemic risk\\nis the primary focus of our paper.\\nMachine learning and market making. Several papers\\nhave applied AI techniques to design automated market mak-\\ners for ﬁnancial markets.1Chan and Shelton [2001 ]focussed\\non the impact of noise from uninformed traders on the quot-\\ning behaviour of a market maker trained with reinforcement\\nlearning. Abernethy and Kale [2013 ]used an online learn-\\ningapproach. Spooner et al. [2018 ]later explored the use\\nof reinforcement learning to train inventory-sensitive market\\nmaking agents in a fully data-driven limit order book model.\\nMost recently, Gu ´eant and Manziuk [2019 ]addressed scaling\\nissues of ﬁnite difference approaches for high-dimensional,\\nmulti-asset market making using model-based RL. While the\\napproach taken in this paper is also based on RL, unlike\\n1A separate strand of work in AI and Economics and Compu-\\ntation has studied automated market makers for prediction markets,\\nsee e.g., Othman [2012 ]. While some similarities to the ﬁnancial\\nmarket making problem pertain, the focus in that strand of work fo-\\ncusses much more on price discovery and information aggregation.the majority of these works, our underlying market model is\\ntaken from the mathematical ﬁnance literature. There, mod-\\nels are typically analysed using methods from optimal con-\\ntrol. We justify this choice in Section 2. To the best of our\\nknowledge, we are the ﬁrst to apply ARL to derive trading\\nstrategies that are robust to epistemic risk.\\nRisk-sensitive reinforcement learning. Risk-sensitivity\\nand safety in RL has been a highly active topic for some\\ntime. This is especially true in robotics where exploration\\nis very costly. For example, Tamar et al. [2012 ]studied pol-\\nicy search in the presence of variance-based risk criteria, and\\nBellemare et al. [2017 ]presented a technique for learning\\nthe full distribution of (discounted) returns; see also Garcıa\\nand Fern ´andez [2015 ]. These techniques are powerful, but', metadata={'source': 'C:\\\\Users\\\\Narex\\\\Downloads\\\\Experimental_Strategies\\\\MM-Research-Papers\\\\main.pdf', 'page': 2}),\n",
       " Document(page_content='Bellemare et al. [2017 ]presented a technique for learning\\nthe full distribution of (discounted) returns; see also Garcıa\\nand Fern ´andez [2015 ]. These techniques are powerful, but\\ncan be complex to implement and can suffer from numer-\\nical instability. This is especially true when using expo-\\nnential utility functions which, without careful considera-\\ntion, may diverge early in training due to large negative re-\\nwards [Maddison et al. , 2017 ]. An alternative approach is\\nto train agents in an adversarial setting [Pinto et al. , 2017;\\nP´erolat et al. , 2018 ]in the form of a zero-sum game. These\\nmethods tackle the problem of epistemic risk2by explicitly\\naccounting for the misspeciﬁcation between train- and test-\\ntime simulations. This robustness to test conditions and ad-\\nversarial disturbances is especially relevant in ﬁnancial prob-\\nlems and motivated the approach taken in this paper.\\n2 Trading Model\\nWe consider a standard model of market making as intro-\\nduced by Avellaneda and Stoikov [2008 ]and studied by many\\nothers including Cartea et al. [2017 ]. The MM trades a sin-\\ngle asset for which the price, Zn, evolves stochastically. In\\ndiscrete-time,\\nZn+1=Zn+bn∆t+σnWn, (1)\\nwherebnandσnare the drift andvolatility coefﬁcients, re-\\nspectively. The randomness comes from the sequence of in-\\ndependent, Normally-distributed random variables, Wn, each\\nwith mean zero and variance ∆t. The process begins with\\ninitial value Z0=zand continues until step Nis reached.\\nThe market maker interacts with the environment at each\\nstep by placing limit orders around Znto buy or sell a single\\nunit of the asset. The prices at which the MM is willing to buy\\n(bid) and sell (ask) are denoted by p+\\nnandp−\\nn, respectively,\\nand may be expressed as offsets from Zn:\\nδ±\\nn=±[Zn−p±\\nn]≥0; (2)\\nthese may be updated at each timestep at no cost to the agent.\\nEquivalently, we may deﬁne:\\nψn=δ+\\nn+δ−\\nn>0,\\npn=1\\n2(p+\\nn+p−\\nn) =Zn+1\\n2(δ−\\nn−δ+\\nn),(3)\\ncalled the quoted spread andreservation price , respectively.\\nThese relate to the agent’s need for immediacy and bias in\\nexecution, amongst other things.\\n2The problem of robustness has also been studied outside of the\\nuse of adversarial learning; see, e.g., [Rajeswaran et al. , 2017 ].', metadata={'source': 'C:\\\\Users\\\\Narex\\\\Downloads\\\\Experimental_Strategies\\\\MM-Research-Papers\\\\main.pdf', 'page': 2}),\n",
       " Document(page_content='In a given time step, the probability that one or both of the\\nagent’s limit orders are executed depends on the liquidity in\\nthe market and the values δ±\\nn. Transactions occur when mar-\\nket orders, which arrive at random times, have sufﬁcient size\\nto consume one of the agent’s limit orders. These interactions\\nare modelled by independent Poisson processes, denoted by\\nN+\\nnandN−\\nnfor the bid and ask sides, respectively, with in-\\ntensitiesλ±\\nn; not to be confused with the terminal timestep\\nN. The dynamics of the agent’s inventory process, or hold-\\nings,Hn, are then captured by the difference between these\\ntwo terms,\\nHn= (N+\\nn−N−\\nn)∈[H,H], (4)\\nwhereH0is known and the values of Hnare constrained so\\nthat trading stops on the opposing side of the book when ei-\\nther limit is reached. The order arrival intensities are given\\nby:\\nλ±\\nn=A±\\nne−k±\\nnδ±\\nn, (5)\\nwhereA±\\nn,k±\\nn>0describe the rate of arrival of market or-\\nders anddistribution of volume in the book , respectively. This\\nparticular form derives from assumptions and observations on\\nthe structure and behaviour of limit order books which we\\nomit here for simplicity; see Avellaneda and Stoikov [2008 ],\\nGould et al. [2013 ], and Abergel et al. [2016 ]for more de-\\ntails.\\nIn this framework, the evolution of the market maker’s cash\\nis given by the difference relation,\\nXn+1=Xn+p−\\nn∆N−\\nn−p+\\nn∆N+\\nn,\\n=Xn+δ−\\nn∆N−\\nn+δ+\\nn∆N+\\nn−Zn∆Hn,(6)\\nwhere ∆N±\\nn≡N±\\nn+1−N±\\nn. The cash ﬂow is a combination\\nof: the proﬁt due to executing at the premium δ±\\nn, and the\\nchange in value of the agent’s holdings. The total value accu-\\nmulated by the agent by timestep nmay thus be expressed as\\nthe sum of the cash held and value invested: Π(Xn,Hn,Zn),\\nwhere\\nΠ(X,H,Z ) =X+HZ, (7)\\nandΠn≡Π(Xn,Hn,Zn). This is known as the mark-to-\\nmarket (MtM) value of the agent’s portfolio.\\nWhy not use a data driven approach? Previous research\\ninto the use of RL for market making — and trading more\\ngenerally — has focussed on data-driven limit order book\\nmodels; see Nevmyvaka et al. [2006 ], Spooner et al. [2018 ],\\nand Vyetrenko and Xu [2019 ]. These methods, however, are\\nnot amenable to the type of analysis presented in Section 3.\\nUsing an analytical model allows us to examine the charac-\\nteristics of adversarial training in isolation while minimising\\nsystematic error due to bias often present in historical data.\\n3 Game formulation and single-stage analysis\\nWe use the market dynamics above to deﬁne a zero-sum\\nstochastic game between a market maker and an adversary\\nthat acts as a proxy for all other market participants.\\nDeﬁnition 1. [Market Making Game] The game between\\nMM and an adversary has Nstages. At each stage, MM\\nchoosesδ±and the adversary {b,A±,k±}. The resultingstage payoff is given by expected change in MtM value of the\\nMM’s portfolio, i.e., E[∆Π] , see (7). The total payoff paid by\\nthe adversary to MM is the sum of the stage payoffs.\\nIn the remainder of this section, we study theoretically the\\ngame when N= 1, i.e., when there is a single stage. Later,\\nin Section 5, we analyse empirically the game for N= 200 .\\nSingle-stage analysis. At each stage, the MM’s payoff may\\nbe unrolled to give:\\nf(δ±;b,A±,k±) =λ+·(δ++b)+λ−·(δ−−b)+bH. (8)\\nFor certain parameter ranges, this equation is concave in δ±.\\nLemma 1 (Payoff Concavity in δ±).The payoff (8)is a con-\\ncave function of δ±on the intervals [0,2/k−b], and [0,2/k+b],\\nrespectively.\\nProof. The ﬁrst derivative of the payoff w.r.t. δ±is given by:\\n∂f\\n∂δ±=λ±[\\n1−k(δ±∓b)]\\n. (9)\\nThe Hessian matrix is thus given by,{\\nk+λ+[k+(δ++b)−2] 0\\n0 k−λ−[k−(δ−−b)−2]}\\n,\\nwhich is negative semi-deﬁnite iff δ±≤2\\nk±±b.\\nNote that (8) is linear in both bandA±. From this, we\\nnext show that there exists a Nash equilibrium (NE) when δ±\\nandbare controlled, and k±andA±are ﬁxed.\\nTheorem 1 (NE for ﬁxed A±,k±).There is a pure strat-\\negy Nash equilibrium (δ±\\n⋆,b⋆)for(δ+,δ−)∈[0,2/k−b]×\\n[0,2/k+b]andb∈[b,b](with ﬁniteb,b),\\nδ±\\n⋆=1\\nk±b⋆;b⋆={bH > 0,\\nb H < 0,(10)\\nwhich is unique for |H|>0. WhenH= 0, there is an', metadata={'source': 'C:\\\\Users\\\\Narex\\\\Downloads\\\\Experimental_Strategies\\\\MM-Research-Papers\\\\main.pdf', 'page': 3}),\n",
       " Document(page_content='egy Nash equilibrium (δ±\\n⋆,b⋆)for(δ+,δ−)∈[0,2/k−b]×\\n[0,2/k+b]andb∈[b,b](with ﬁniteb,b),\\nδ±\\n⋆=1\\nk±b⋆;b⋆={bH > 0,\\nb H < 0,(10)\\nwhich is unique for |H|>0. WhenH= 0, there is an\\nequilibrium for every value b⋆∈[b,b].\\nProof. Equating (9) to zero and solving gives (10). To prove\\nthat these correspond to a pure strategy Nash Equilibrium of\\nthe game we show that the payoff is quasi-concave (resp.\\nquasi-convex) in the MM’s (resp. adversary’s) strategy and\\nthen apply Sion’s minimax theorem [Sion, 1958 ]. This fol-\\nlows from Lemma 1 for MM, and by the linearity of the ad-\\nversary’s payoff w.r.t. b. For|H|>0, there is a unique solu-\\ntion. WhenH= 0, there exists a continuum of solutions, all\\nwith equal payoff.\\nThe solution (10) has a similar form to that of the optimal\\nstrategy for linear utility with terminal inventory penalty [Fo-\\ndra and Labadie, 2012 ], or equivalently that of a myopic agent\\nwith running penalty [Cartea and Jaimungal, 2015 ]. Interest-\\ningly, the extension of Theorem 1 to an adversary with control\\nover all three model parameters {b,A±,k±}yields a similar\\nresult. In this case we refer the reader the extended version of\\nthe paper [Spooner and Savani, 2020 ]for a proof, and leave\\nuniqueness to future work.\\nTheorem 2 (NE for general case) .There exists a pure strat-\\negy Nash equilibrium (δ±\\n⋆,{b⋆,A±\\n⋆,k±\\n⋆})of the MM game\\nfor(10),A±\\n⋆=Aandk±\\n⋆=k.', metadata={'source': 'C:\\\\Users\\\\Narex\\\\Downloads\\\\Experimental_Strategies\\\\MM-Research-Papers\\\\main.pdf', 'page': 3}),\n",
       " Document(page_content='4 Adversarial training\\nThe single-stage setting is informative but unrealistic. Next\\nwe investigate a range of multi-stage settings with various\\ndifferent restrictions on the adversary, and explore how adver-\\nsarial training can improve the robustness of MM strategies.\\nThe following three types of adversary in turn increase the\\nfreedom of the adversary to control the market’s dynamics:\\nFIXED .The simplest possible adversary always plays the\\nsame ﬁxed strategy, for us: bn= 0,A±\\nn= 140 andk±\\nn= 1.5;\\nthese values match those originally used by Avellaneda and\\nStoikov [2008 ]. This amounts to a single-agent learning set-\\nting with stationary transition dynamics .\\nRANDOM .The second type of adversary instantiates each\\nepisode with parameters chosen independently and uniformly\\nat random from the following ranges: bn=b∈[−5,5],\\nA±\\nn=A∈[105,175] andk±\\nn=k∈[1.125,1.875]. These\\nare chosen at the start of each episode and remain ﬁxed until\\nthe terminal timestep N. This is analogous to single-agent\\nRL with non-stationary transition dynamics .\\nSTRATEGIC .The ﬁnal type of adversary chooses the model\\nparametersbn,An,kn(bounded as in the previous setting) at\\neach step of the game . This represents a fully-adversarial and\\nadaptive learning environment , and unlike the models pre-\\nsented in related work [Cartea et al. , 2017 ], the source of risk\\nhere is exogenous andreactive to the quotes of the MM.\\nThe principle of adversarial learning here — as with other\\nsuccessful applications [Goodfellow et al. , 2014 ]— is that\\nif the MM plays a strategy that is not robust, then this can\\n(and ideally will) be exploited by the adversary. If a Nash\\nequilibrium (NE) strategy is played by MM then their strat-\\negy is robust and cannot be exploited. While there are no\\nguarantees that a NE will be reached in our case, we show\\nin Section 5 via empirical best response computations that\\nour ARL method does consistently converge to reasonable\\napproximate NE. Moreover, we show that we consistently\\noutperform previous approaches in terms of absolute perfor-\\nmance androbustness to model ambiguity.\\nRobustness through ARL was ﬁrst introduced by Pinto et\\nal.[2017 ]who demonstrated its effectiveness across a num-\\nber of standard OpenAI gym domains. We adapt their RARL\\nalgorithm to support incremental actor-critic based methods\\nand facilitate asynchronous training ; though many of the fea-\\ntures remain the same. The adversary is trained in parallel\\nwith the market maker, is afforded the same access to state —\\nincluding MM’s inventory Hn— and uses the same method\\nfor learning, which is described below.\\n4.1 Learning conﬁguration\\nBoth agents use the NAC-S( λ) algorithm, a natural actor-\\ncritic method [Thomas, 2014 ]for stochastic policies (i.e.,\\nmixed strategies) using semi-gradient SARSA( λ)[Rummery\\nand Niranjan, 1994 ]for policy evaluation. The value func-\\ntions are represented by compatible [Peters and Schaal, 2008 ]\\nradial basis function networks of 100 Gaussian prototypes\\nwith accumulating eligibility traces [Sutton and Barto, 2018 ].\\nStates. The state of the environment sn= (tn,Hn)contains\\nonly the current time tn=nT\\nN=n∆tand the agent’s inven-toryHn, where the transition dynamics are governed by the\\ndeﬁnitions introduced in Section 2.\\nPolicies. The market maker learns a bivariate Normal pol-\\nicy for˜pn≡pn−Snandψnwith diagonal covariance\\nmatrix. The mean and variance vectors are modelled by\\nlinear function approximators using 3rd-order polynomial\\nbases [Lagoudakis and Parr, 2003 ]. Both variances and the\\nmean of the spread term, ψn, are kept positive via a softplus\\ntransformation. The adversary learns a Beta policy [Chou et\\nal., 2017 ], shifted and scaled to cover the market parameter\\nintervals. The two shape parameters are learnt the same as for\\nthe variance of the Normal distribution above, with a transla-\\ntion of +1to ensure unimodality and concavity.\\nRewards. The reward function is an adaptation of the opti-', metadata={'source': 'C:\\\\Users\\\\Narex\\\\Downloads\\\\Experimental_Strategies\\\\MM-Research-Papers\\\\main.pdf', 'page': 4}),\n",
       " Document(page_content='the variance of the Normal distribution above, with a transla-\\ntion of +1to ensure unimodality and concavity.\\nRewards. The reward function is an adaptation of the opti-\\nmisation objective of Cartea et al. [2017 ]and others, for the\\nRL setting of incremental, discrete-time feedback:\\nRn= ∆Πn−ζH2\\nn−{0 fort<T,\\nηH2\\nn otherwise,(11)\\nwhere Πnrefers to the MtM value of the agent’s holdings\\n(Eq. 7). This formulation can be either risk-neutral (RN) or\\nrisk-adverse (RA). For example, if η>0andζ= 0, then the\\nagent is punished if the terminal inventory HNis non-zero,\\nand the solution becomes time-dependent. We refer to this\\ncase as RA and the case of η=ζ= 0as RN.\\n5 Experiments\\nIn each of the experiments to follow, the value function was\\npre-trained for 1000 episodes (with a learning rate of 10−3) to\\nreduce variance in early policy updates. Both the value func-\\ntion and policy were then trained for 106episodes, with pol-\\nicy updates every 100 time steps, and a learning rate of 10−4\\nfor both the critic and policy. The value function was conﬁg-\\nured to learn λ= 0.97returns. The starting time was chosen\\nuniformly at random from the interval t0∈[0.0,0.95], with\\nstarting price Z0= 100 and inventory H0∈[H=−50,H=\\n50]. Innovations in Znoccurred with ﬁxed volatility σ= 2\\nbetween [t0,1]with increment ∆t= 0.005.\\n5.1 Market maker desiderata\\nMarket makers typically aim to maximise proﬁts while con-\\ntrolling inventory and quoted spread. Higher terminal wealth\\nwith lower variance, less exposure to inventory risk and\\nsmaller spreads are all indicators of the risk aversion (or lack\\nthereof) of a strategy. This is reﬂected in the majority of ob-\\njective functions for market markers studied in the literature.\\nWe consider the following quantitative desiderata:\\nProﬁt and loss / Sharpe Ratio. The distribution of a trading\\nstrategy’s proﬁt and loss is the most fundamental object used\\nto measure its performance. In our setup, we look at an\\nagent’s distribution of episodic terminal wealth. In particu-\\nlar, we look at the moments E[ΠN]andV[ΠN]of this dis-\\ntribution, where the agent would like to maximise the former\\nwhile minimising the latter. To this end, a common metric\\nused in ﬁnancial literature and in industry is the Shape ratio:', metadata={'source': 'C:\\\\Users\\\\Narex\\\\Downloads\\\\Experimental_Strategies\\\\MM-Research-Papers\\\\main.pdf', 'page': 4}),\n",
       " Document(page_content='(a) Reservation price offset, ˜p(t,H).\\n (b) Quoted spread, ψ(t,H).\\nFigure 1: Most probable action for the risk-averse Gaussian policy learnt using NAC-S( λ) withη= 0andζ= 0.01.\\nTable 1: MM performance in terms of the standard desiderata. Each\\nvalue was computed by evaluating the policy over 105episodes\\nand computing the mean and sample standard deviation. Each test\\nepisode was generated with a F IXED adversary. Wealth and average\\nspread are measured in units of “currency”; inventory in terms of\\nunits of the “asset”; and the Sharpe ratio is unitless.\\n(a) Market makers trained against the F IXED adversary.\\nη ζ Term. wealth Sharpe Term. inventory Avg. spread\\n0.0 0.0 67.0±12.0 5.57 0.56±7.55 1.42±0.02\\n1.0 0.0 61.3±8.5 7.25−0.04±1.19 1.76±0.02\\n0.5 0.0 63.0±9.9 6.34 0.03±1.24 1.67±0.03\\n0.1 0.0 66.4±9.4 7.06−0.16±1.86 1.42±0.02\\n0.01 0.0 66.8±11.1 6.00 0.87±4.38 1.42±0.02\\n0.0 0.01 63.4±6.8 9.36 0.01±1.45 1.60±0.02\\n0.0 0.001 66.1±7.4 8.94 0.15±2.97 1.44±0.02\\n(b) Market makers trained against the R ANDOM adversary.\\nη ζ Term. wealth Sharpe Term. inventory Avg. spread\\n0.0 0.0 66.7±11.8 5.65 0.38±7.14 1.36±0.05\\n1.0 0.0 59.4±7.6 7.79 0.02±1.09 1.87±0.02\\n0.5 0.0 62.9±8.4 7.51−0.04±1.21 1.68±0.02\\n0.1 0.0 65.8±9.3 7.07−0.18±1.57 1.46±0.03\\n0.01 0.0 66.6±9.9 6.69−0.65±3.92 1.47±0.02\\n0.0 0.01 64.0±6.7 9.54−0.06±1.31 1.60±0.02\\n0.0 0.001 65.9±7.2 9.11−0.07±2.62 1.44±0.02\\n(c) Market makers trained against a S TRATEGIC adversary.\\nTerm. wealth Sharpe Term. inventory Avg. spread\\nRNb 65.5±6.9 9.51 0.04±2.14 1.43±0.01\\nA±66.9±13.1 5.11 2.45±7.76 1.46±0.02\\nk±66.9±12.7 5.27 0.52±7.92 1.45±0.02\\nb,A±,k±65.1±6.7 9.78−0.05±1.94 1.44±0.02\\nFullη= 1.0 60.5±6.8 8.88−0.02±0.97 1.75±0.02\\nη= 0.5 63.3±6.8 9.32−0.07±1.05 1.60±0.01\\nη= 0.1 64.8±6.7 9.72−0.06±1.37 1.49±0.02\\nη= 0.01 65.0±6.7 9.69−0.08±1.89 1.49±0.01\\nζ= 0.01 62.9±6.7 9.43−0.03±1.19 1.65±0.02\\nζ= 0.001 64.3±6.5 9.85 0.0±1.71 1.44±0.01E[ΠN]\\nV[ΠN], in essence, the reward per unit of risk.3While larger\\nvalues are better, it is important to note that the Sharpe ratio\\nis not a sufﬁcient statistic.\\nTerminal inventory. The distribution of terminal inventory,\\nHN, tells us about the robustness of the strategy to adverse\\nprice movements. In practice, it is desirable for a market\\nmaker to ﬁnish the trading day with small absolute values for\\nHN; though this is not always a strict requirement.\\nQuoted spread. The “competitiveness” of a market maker\\nis often discussed in terms of the average quoted spread:\\nEn[ψn]. Tighter spreads imply more efﬁcient markets and ex-\\nchanges often compensate (with rebates) for smaller spreads.\\n5.2 Results\\nFIXED setting. We ﬁrst trained MMs against the F IXED ad-\\nversary; i.e., a standard single-agent learning environment.\\nBoth RN and RA formulations of Eq. 11 were used with risk\\nparametersη∈ {1,0.5,0.1,0.01}andζ∈ {0.01,0.001}.\\nTable 1a summarises the performance for the resulting agents.\\nTo provide some further intuition, we illustrate one of the\\nlearnt policies in Figure 1. In this case, the agent learnt to\\noffset its price asymmetrically as a function of inventory, with\\nincreasing intensity as we approach the terminal time.\\nRANDOM setting. Next MMs were trained in an environ-\\nment with a R ANDOM adversary; a simple extension to the\\ntraining procedure that aims to develop robustness to epis-\\ntemic risk. To compare with earlier results, the strategies\\nwere also tested against the F IXED adversary — a summary\\nof which, for the same set of risk parameters, is given in Ta-\\nble 1b. The impact on test performance in the face of model\\nambiguity was then evaluated by comparing market makers\\ntrained on the F IXED adversary with those trained against the\\nRANDOM adversary. Speciﬁcally, out-of-sample tests were\\ncarried out in an environment with a R ANDOM adversary.\\nThis means that the model dynamics at test-time were differ-\\nent from those at training time. While not explicitly adversar-\\nial, this misspeciﬁcation of the model represents a non-trivial', metadata={'source': 'C:\\\\Users\\\\Narex\\\\Downloads\\\\Experimental_Strategies\\\\MM-Research-Papers\\\\main.pdf', 'page': 5}),\n",
       " Document(page_content='This means that the model dynamics at test-time were differ-\\nent from those at training time. While not explicitly adversar-\\nial, this misspeciﬁcation of the model represents a non-trivial\\nchallenge for robustness. Overall, we found that market mak-\\ners trained against the F IXED adversary exhibited no change\\nin average wealth creation, but an increase of 98.1% in the\\n3While the Sharpe ratio would normally use returns, for our rel-\\native comparison of agents we use terminal wealth.', metadata={'source': 'C:\\\\Users\\\\Narex\\\\Downloads\\\\Experimental_Strategies\\\\MM-Research-Papers\\\\main.pdf', 'page': 5}),\n",
       " Document(page_content='Figure 2: Policy learnt by the adversary for manipulating price\\nagainst the market maker with η=ζ= 0.\\nvariance across all risk parametrisations. On the other hand,\\nmarket makers originally trained against the R ANDOM adver-\\nsary yielded a lower average 86.0% increase in the variance.\\nThe R ANDOM adversary helps, but not by much compared\\nwith the S TRATEGIC adversary, as we will see next.\\nSTRATEGIC setting. In this setting, we ﬁrst consider an\\nadversary that controls the drift bonly. With RN rewards,\\nwe found that the adversary learns a (time-independent) bi-\\nnary policy (Figure 2) that is identical to the strategy in the\\ncorresponding single-stage game; see Section 3. We also\\nfound that the strategy learnt by the MM in this setting gener-\\nates proﬁts and associated Sharpe ratios in excess of all other\\nstrategies seen thus far when tested against the F IXED adver-\\nsary (see Table 1c). This is true also when comparing with\\ntests run against the R ANDOM or S TRATEGIC adversaries,\\nsuggesting that the adversarially trained MM is indeed more\\nrobust to test-time model discrepancies.\\nThis, however, does not extend to S TRATEGIC adversaries\\nwith control over either onlyA±oronlyk±. In these cases,\\nperformance was found to be no better than the corresponding\\nMMs trained in the F IXED setting with a conventional learn-\\ning setup. The intuition for this again derives from the single-\\nstage analysis. That is, the adversary almost surely chooses a\\nstrategy that minimises A±(equiv. maximises k±) in order to\\ndecrease the probability of execution, thus decreasing MM’s\\nproﬁts derived from execution and MM’s ability to manage\\ninventory effectively. The control afforded to the adversary\\nmust correlate in some way with sources of variance — such\\nas inventory speculation — in order for robustness to corre-\\nspond to practicable forms of risk-aversion.\\nThen the natural question is then whether an adversary with\\nsimultaneous control over b,A±, andk±produces strategies\\noutperforming those where the adversary controls balone.\\nThis is plausible since combining the three model parameters\\ncan lead to more interesting strategies, e.g., driving inventory\\nup/down only to increase drift at the peak (i.e., pump and\\ndump). We investigated by training an adversary with control\\nover all three parameters. The resulting performance can be\\nfound in Table 1c, which shows an improvement in the Sharpe\\nratio of 0.27 and lower variance on terminal wealth. Interest-\\ningly, these MMs also quote tighter spreads on average — the\\nvalues even approaching that of the risk-neutral MM trained\\nagainst a F IXED adversary. This indicates that the strategies\\nare able to achieve epistemic risk aversion without charging\\nmore to counterparties.Exploring the impact of varying risk parameters of the\\nreward function, ηandζ(see (11)), we found that in all\\ncases MM strategies trained against a S TRATEGIC adversary\\nwith an RA reward outperformed their counterparts in Ta-\\nbles 1a and 1b. It is unclear, however, if changes to the re-\\nward function away from the RN variant actually improved\\nthe strategy in general. Excluding when ζ= 0.001, all val-\\nues appear to do worse than for an adversarially trained MM\\nwith RN reward. It may well be that the addition of inventory\\npenalty terms actually boosts the power of the adversary and\\nresults in strategies that try to avoid trading at all, a frequent\\nproblem in this domain.\\nVeriﬁcation of approximate equilibria. Holding the strat-\\negy of one player ﬁxed, we empirically computed the best\\nresponse against it by training. We found consistently that\\nneither the trader nor adversary deviated from their policy.\\nThis suggests that our ARL method were ﬁnding reasonable\\napproximate Nash equilibria. While we do not provide a\\nfull theoretical analysis of the stochastic game, these ﬁnd-\\nings are corroborated by those in Section 3, since, as seen\\nin Figure 2, the learned policy in the multi-stage setting cor-', metadata={'source': 'C:\\\\Users\\\\Narex\\\\Downloads\\\\Experimental_Strategies\\\\MM-Research-Papers\\\\main.pdf', 'page': 6}),\n",
       " Document(page_content='full theoretical analysis of the stochastic game, these ﬁnd-\\nings are corroborated by those in Section 3, since, as seen\\nin Figure 2, the learned policy in the multi-stage setting cor-\\nresponds closely to the equilibrium strategy from the single-\\nstage case that was presented in Section 3.\\n6 Conclusions\\nWe introduced a new approach for learning trading strate-\\ngies with ARL. The learned strategies are robust to the dis-\\ncrepancies between the market model in training and testing.\\nWe show that our approach leads to strategies that outper-\\nform previous methods in terms of PnL and Sharpe ratio, and\\nhave comparable spread efﬁciency. This is shown to be the\\ncase for out-of-sample tests in all three of the considered set-\\ntings, F IXED , RANDOM , and S TRATEGIC . In other words,\\nour learned strategies are not only more robust to misspeciﬁ-\\ncation, but also dominate in overall performance.\\nIn some special cases we show that the learned strategies\\ncorrespond to Nash equilibria of the corresponding single-\\nstage game. More widely, we empirically show that the\\nlearned strategies correspond to approximate equilibria in the\\nmulti-stage stochastic game.\\nIn future we plan to:\\n1. Extend to oligopolies of market makers.\\n2. Apply to data-driven and multi-asset models.\\n3. Further explore existence of equilibria and design\\nprovably-convergent algorithms.\\nFinally, we remark that, while our paper focuses on market\\nmaking, the approach can be applied to other trading scenar-\\nios such as optimal execution and statistical arbitrage, where\\nwe believe it is likely to offer similar beneﬁts. Further, it\\nis important to acknowledge that this methodology has sig-\\nniﬁcant implications for safety of RL in ﬁnance. Training\\nstrategies that are explicitly robust to model misspeciﬁcation\\nmakes deployment in the real-world considerably more prac-\\nticable.\\nSoftware. All our code is freely accessible on GitHub:\\nhttps://github.com/tspooner/rmm.arl.', metadata={'source': 'C:\\\\Users\\\\Narex\\\\Downloads\\\\Experimental_Strategies\\\\MM-Research-Papers\\\\main.pdf', 'page': 6}),\n",
       " Document(page_content='References\\n[Abergel et al. , 2016 ]F. Abergel, M. Anane, A. Chakraborti, A. Je-\\ndidi, and I. M. Toke. Limit Order Books . Cambridge University\\nPress, 2016.\\n[Abernethy and Kale, 2013 ]J. D. Abernethy and S. Kale. Adaptive\\nMarket Making via Online Learning. In Proc. of NIPS , 2013.\\n[Avellaneda and Stoikov, 2008 ]M. Avellaneda and S. Stoikov.\\nHigh-frequency trading in a limit order book. Quantitative Fi-\\nnance , 8(3):217–224, 2008.\\n[Bellemare et al. , 2017 ]M. G. Bellemare, W. Dabney, and R.\\nMunos. A Distributional Perspective on Reinforcement Learn-\\ning. In Proc. of ICML , 2017.\\n[Cartea and Jaimungal, 2015 ]´A. Cartea and S. Jaimungal. Order-\\nFlow and Liquidity Provision. Available at SSRN 2553154 , 2015.\\n[Cartea et al. , 2015 ]´A. Cartea, S. Jaimungal, and J. Penalva. Al-\\ngorithmic and High-Frequency Trading . Cambridge University\\nPress, 2015.\\n[Cartea et al. , 2017 ]´A. Cartea, R. Donnelly, and S. Jaimungal. Al-\\ngorithmic Trading with Model Uncertainty. SIAM Journal on\\nFinancial Mathematics , 8(1):635–671, 2017.\\n[Chan and Shelton, 2001 ]N. T. Chan and C. Shelton. An Electronic\\nMarket-Maker. 2001.\\n[Chou et al. , 2017 ]P.-W. Chou, D. Maturana, and S. Scherer. Im-\\nproving Stochastic Policy Gradients in Continuous Control with\\nDeep Reinforcement Learning Using the Beta Distribution. In\\nProc. of ICML , 2017.\\n[Fodra and Labadie, 2012 ]P. Fodra and M. Labadie. High-\\nfrequency market-making with inventory constraints and direc-\\ntional bets. arXiv preprint arXiv:1206.4810 , 2012.\\n[Garcıa and Fern ´andez, 2015 ]J. Garcıa and F. Fern ´andez. A com-\\nprehensive survey on safe reinforcement learning. JMLR ,\\n16(1):1437–1480, 2015.\\n[Glosten and Milgrom, 1985 ]L. R. Glosten and P. R. Milgrom.\\nBid, Ask and Transaction Prices in a Specialist Market with\\nHeterogeneously Informed Traders. Journal of Financial Eco-\\nnomics , 14(1):71–100, 1985.\\n[Goodfellow et al. , 2014 ]I. Goodfellow, J. Pouget-Abadie, M.\\nMirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y .\\nBengio. Generative Adversarial Nets. In Proc. of NIPS , 2014.\\n[Gould et al. , 2013 ]M. D. Gould, M. A. Porter, S. Williams, M.\\nMcDonald, D. J. Fenn, and S. D. Howison. Limit Order Books.\\nQuantitative Finance , 13(11):1709–1742, 2013.\\n[Grossman and Miller, 1988 ]S. J. Grossman and M. H. Miller. Liq-\\nuidity and Market Structure. The Journal of Finance , 43(3):617–\\n633, 1988.\\n[Gu´eant and Manziuk, 2019 ]O. Gu ´eant and I. Manziuk. Deep rein-\\nforcement learning for market making in corporate bonds: beat-\\ning the curse of dimensionality. arXiv:1910.13205 , 2019.\\n[Gu´eant et al. , 2011 ]O. Gu ´eant, C.-A. Lehalle, and J. Fernandez-\\nTapia. Dealing with the Inventory Risk: A solution to the mar-\\nket making problem. Mathematics and Financial Economics ,\\n7(4):477–507, 2011.\\n[Gu´eant, 2017 ]O. Gu ´eant. Optimal market making. Applied Math-\\nematical Finance , 24(2):112–154, 2017.\\n[Ho and Stoll, 1981 ]T. Ho and H. R. Stoll. Optimal Dealer Pricing\\nUnder Transactions and Return Uncertainty. Journal of Financial\\nEconomics , 9(1):47–73, 1981.[Lagoudakis and Parr, 2003 ]M. G. Lagoudakis and R. Parr. Least-\\nSquares Policy Iteration. JMLR , 4:1107–1149, 2003.\\n[Littman, 1994 ]M. L. Littman. Markov games as a framework for\\nmulti-agent reinforcement learning. In Proc. of ICML . 1994.\\n[Maddison et al. , 2017 ]C. J. Maddison, D. Lawson, G. Tucker, N.\\nHeess, M. Norouzi, A. Mnih, A. Doucet, and Y . W. Teh. Particle\\nValue Functions. In ICLR 2017 Workshop Proceedings , 2017.\\n[Nevmyvaka et al. , 2006 ]Y . Nevmyvaka, Y . Feng, and M. Kearns.\\nReinforcement Learning for Optimized Trade Execution. In\\nProc. of ICML , 2006.\\n[Othman, 2012 ]A. Othman. Automated Market Making: Theory\\nand Practice . PhD thesis, CMU, 2012.\\n[P´erolat et al. , 2018 ]J. P´erolat, B. Piot, and O. Pietquin. Actor-\\nCritic Fictitious Play in Simultaneous Move Multistage Games.\\nInProc. of AISTATS , 2018.\\n[Peters and Schaal, 2008 ]J. Peters and S. Schaal. Natural Actor-\\nCritic. Neurocomputing , 71(7-9):1180–1190, 2008.', metadata={'source': 'C:\\\\Users\\\\Narex\\\\Downloads\\\\Experimental_Strategies\\\\MM-Research-Papers\\\\main.pdf', 'page': 7}),\n",
       " Document(page_content='InProc. of AISTATS , 2018.\\n[Peters and Schaal, 2008 ]J. Peters and S. Schaal. Natural Actor-\\nCritic. Neurocomputing , 71(7-9):1180–1190, 2008.\\n[Pinto et al. , 2017 ]L. Pinto, J. Davidson, R. Sukthankar, and A.\\nGupta. Robust Adversarial Reinforcement Learning. In Proc.\\nof ICML , 2017.\\n[Rajeswaran et al. , 2017 ]A. Rajeswaran, S. Ghotra, B. Ravindran,\\nand S. Levine. EPOpt: Learning Robust Neural Network Policies\\nusing Model Ensembles. In Proc. of ICLR , 2017.\\n[Rummery and Niranjan, 1994 ]G. A. Rummery and M. Niranjan.\\nOn-line Q-learning Using Connectionist Systems. Technical re-\\nport, Department of Engineering, University of Cambridge, 1994.\\n[Sion, 1958 ]M. Sion. On General Minimax Theorems. Paciﬁc\\nJournal of Mathematics , 8(1):171–176, 1958.\\n[Spooner and Savani, 2020 ]T. Spooner and R. Savani. Robust Mar-\\nket Making via Adversarial Reinforcement Learning. arXiv\\npreprint arXiv:2003.01820 , 2020.\\n[Spooner et al. , 2018 ]T. Spooner, J. Fearnley, R. Savani, and A.\\nKoukorinis. Market Making via Reinforcement Learning. In\\nProc. of AAMAS , pages 434–442, 2018.\\n[Sutton and Barto, 2018 ]R. S. Sutton and A. G. Barto. Reinforce-\\nment Learning: An Introduction . MIT Press, 2018.\\n[Tamar et al. , 2012 ]A. Tamar, D. Di Castro, and S. Mannor. Policy\\nGradients with Variance Related Risk Criteria. In Proc. of ICML ,\\n2012.\\n[Thomas, 2014 ]P. Thomas. Bias in Natural Actor-Critic Algo-\\nrithms. In Proc. of ICML , 2014.\\n[Vyetrenko and Xu, 2019 ]S. Vyetrenko and S. Xu. Risk-Sensitive\\nCompact Decision Trees for Autonomous Execution in Presence\\nof Simulated Market Response. arXiv:1906.02312 , 2019.\\nView publication stats', metadata={'source': 'C:\\\\Users\\\\Narex\\\\Downloads\\\\Experimental_Strategies\\\\MM-Research-Papers\\\\main.pdf', 'page': 7})]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cdfa35-41ca-477c-96e7-c35d611d1711",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
